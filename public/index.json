[{"content":" Table of Contents 1. Continuous Training 2. Training Pipeline 2.1. Training 2.2. Evaluation 3. Deployment 1. Continuous Training The process of generating a machine learning model usually consists of training on a dataset, evaluating its performance and then deploying it to a production environment. In certain scenarios, the trained model may perform consistently over a long period of time. However, that is not always the case. There are multiple reasons you may want to retrain it periodically, such as:\nStay competitive: as more data becomes available, the model can be retrained to take advantage of it, gaining a competitive edge thanks to the improved performance. Data drift: in training a model, it is assumed that the training data is representative of future unseen data. Nevertheless, this may not always hold. For instance, we trained a model to estimate the camera parameters for broadcast soccer games. But when the pandemic led to camera angle changes in order to avoid showing empty stadiums, the data distribution changed. Concept drift: the relationship between the input features and the target variable can evolve. We trained our camera model to predict the camera pinhole parametrisation. The model we trained our model to predict the parametrisation given by the camera pinhole paradigm. However, if TV producers switch to fish-eye lenses for a different viewer experience, the input/output mapping is no longer be valid. The process of retraining a model on a regular basis is known as Continuous Training. It is a key component of a machine learning system that aims to keep the model\u0026rsquo;s performance at an optimal level over time. The following diagram depicts the different components that make up a Continuous Training System:\nDiagram for the Continuous Training System we will be exploring in this article. It depicts the different components it is comprised of, and the tasks they carry out. We have the following agents/components:\nUser: customer that sends requests to the endpoint in order to get predictions back. Endpoint: the model\u0026rsquo;s API, which receives forwards the requests to the model and sends back the predictions. Furthermore, it stores both the pairs of queries and predictions to the data lake. Labelling: process used to manually annotate the data and review the model\u0026rsquo;s predictions. This allows us to have high quality data on a continuous basis that we can use to capture the data drift and retrain the model. Data Lake: storage system that stores the high quality data used for training. Orchestrator: component that watches the data lake and triggers the retraining process when certain conditions are met. Training: process that takes the data in the data lake and generates a new model as an output artifact. In this article we will focus on the Training and Endpoint components. All the code is available at the following public repository:\nAuxiliary repository Note: in this series we will build from scratch a Continuous Training System. However, to keep things simple, we will use a toy example and run the system locally. In a real-world scenario, the system would be deployed in a cloud environment and the data would be stored in a distributed storage system. 2. Training Pipeline In this section we will focus on the Training component. It carries out a sequence of steps that takes the data in the data lake and generates a new model as an output artifact. The pipeline is composed of the following steps:\nTraining: the process of learning the best weights that minimize the error between the predictions and the ground truth in the training set, while being able to generalize to unseen data. Evaluation: the process of comparing the performance of the newly trained model against a baseline model in order to find out if there is an improvement. Validation: a final step to ensure that the model is able to generalize to unseen data. Registry: if all previous steps are successful, the model is stored, ready to be deployed. Diagram for the Continuous Training System we will be exploring in this article. It depicts the different components it is comprised of, and the tasks they carry out. Lets us now focus on the two key components that carry out the core functionality of the pipeline:\n2.1. Training We will try to keep things as simple as possible. In that spirit, our aim is to build a Dog \u0026amp; Cat classifier. The starting dataset can be found in the data folder consisting of:\nImages: 10 comprised of 10 photos of cats and dogs, located in data subfolder. Labels: 10 corresponding labels stored as .json, located in annotations subfolder. Example of a cat and a dog images in our dataset. We will be using the torch library. The main components we defined are:\nDataset: reads the images and labels from the disk and returns them as a tuple. It resizes the images to a fixed size, converts them to grayscale and normalizes them. Model: defines the model architecture, which consists of a simple fully connected layer followed by a sigmoid activation function. Loss: we will use the binary cross-entropy loss. To see the training in practice, you can download a sample dataset here. Extract the contents of the file in the data directory. To do so, run:\ntar -zxf cats_and_dogs.tar.gz -C data/ Then make sure you have poetry installed and set up the environment running:\npoetry install First, we need prepare the dataset by creating the 3 splits for train/val/test:\npoetry run python -m animal_classifier dataset-split data/train.csv data/val.csv data/test.csv --train-frac 0.6 --val-frac 0.2 --test-frac 0.2 We will use the train set to fine tune the model weights, and the val set for early stopping. Run the following command to get the trained model, which will be stored in the models folder:\npoetry run python -m animal_classifier training data/train.csv data/val.csv --annotations-dir data/cats_and_dogs/annotations --model-dir models/cats_and_dogs The model is now ready to be used for inference. To test it on an image, simply run\npoetry run python -m animal_classifier inference 1.png base_model.pth --frames-dir data/cats_and_dogs/frames --model-dir models/cats_and_dogs --threshold 0.5 2.2. Evaluation Before deploying the model, we need to evaluate its performance. For that purpose, we have an evaluation module that receives a model and a dataset. It computes a summary metric, the average binary cross-entropy loss. We will carry out two steps:\nComparison against a baseline model: to ensure the new model outperforms the current one, we compare the metric on the same dataset. As an example, we can compare the two models provided by the training step in previous section: latest (all epochs) and best (early stopping). poetry run python -m animal_classifier evaluation base_model_latest.pth base_model.pth data/test.csv --annotations-dir data/cats_and_dogs/annotations --model-dir models/cats_and_dogs Validation: to ensure the model is able to generalize to unseen data. poetry run python -m animal_classifier validation base_model.pth data/test.csv --annotations-dir data/cats_and_dogs/annotations --model-dir models/cats_and_dogs --max-loss-validation 5 3. Deployment So after training our model, making sure it outperforms the baseline and gaining confidence about its generalization to unseen data, what\u0026rsquo;s next? It\u0026rsquo;s time to deploy it!\nBear in mind that our model is simply an artifact, a file that contains the weights of the model. In order to make predictions, we need to wrap it in an API. We will use the torchserve library. The process goes as follows:\nModel Archiver: we need to create a .mar file that contains the model and the code to run it: poetry run torch-model-archiver --model-name animal --version 1.0 --model-file animal_classifier/models/model.py --serialized-file ./models/cats_and_dogs/base_model.pth --handler animal_classifier/api/torchserve/handler.py --export-path ./model_store/ Settomgs: we define the settings for the server in a config.properties file, like the one below: inference_address=http://127.0.0.1:8080 inference_address=http://0.0.0.0:8080 management_address=http://0.0.0.0:8081 metrics_address=http://0.0.0.0:8082 model_store=./model_store/ load_models=animal.mar min_workers=1 max_workers=1 default_workers_per_model=1 model_snapshot={ \u0026#34;name\u0026#34;:\u0026#34;startup.cfg\u0026#34;, \u0026#34;modelCount\u0026#34;:1, \u0026#34;models\u0026#34;:{ \u0026#34;animal\u0026#34;:{ \u0026#34;1.0\u0026#34;:{ \u0026#34;defaultVersion\u0026#34;:true, \u0026#34;marName\u0026#34;:\u0026#34;animal.mar\u0026#34;, \u0026#34;minWorkers\u0026#34;:1, \u0026#34;maxWorkers\u0026#34;:1, \u0026#34;batchSize\u0026#34;:2, \u0026#34;maxBatchDelay\u0026#34;:5000, \u0026#34;responseTimeout\u0026#34;:30000, } } } } ","permalink":"http://localhost:1313/posts/projective_geometry/building_training_pipeline/","summary":"Table of Contents 1. Continuous Training 2. Training Pipeline 2.1. Training 2.2. Evaluation 3. Deployment 1. Continuous Training The process of generating a machine learning model usually consists of training on a dataset, evaluating its performance and then deploying it to a production environment. In certain scenarios, the trained model may perform consistently over a long period of time. However, that is not always the case. There are multiple reasons you may want to retrain it periodically, such as:","title":"üèãÔ∏è Continuous Training: Building a model training pipeline"},{"content":" Table of Contents 1. Introduction 2. Via geometric features 2.1. From points/lines 2.1.1. Problem formulation 2.1.2. Noise amplification: the horizon line 2.1.3. Solution: least squares estimator 2.2. From conics 2.3. From multiple features 3. Via a pair images 4. Via ML model 5. Summary 6. References 1. Introduction So far, we have figured out how to:\nMathematically characterize the transform between the 3D world and a 2D image of it Map different types of objects between the two domains However, one might wonder: how do we actually compute the homography matrix that fully describes that transform in the first place? In this post, we will explain different approaches to do precisely that.\n‚ö†Ô∏è There are multiple ways to characterize the perspective transform. In this post we will focus on estimating the homography matrix, as oposed to the more interpretable KRT parametrisation (focal length, rotation angles and 3D location). The reason for that choice is the implicit assumption that we are only given a 2D image to characterize the transform. Under that constraint, the KRT parametrisation is not resolvable, as illustrated in the image below. If we have a zenithal view of the pitch, we could end up capturing the exact same image by varying accordingly the distance to the ground and the focal length. To resolve the ambiguity, we would need correspondences between points in the captured 2D image and non-coplanar points in the 3D world. Depiction of a soccer field photographed by two different cameras from a zenithal view. By adjusting the focal length, it is possible to capture the exact same image of the field from both angles. This illustrates the ambiguity in trying to retrieve the camera parameters from a 2D image. Auxiliary repository 2. Via geometric features We have explained in an earlier post how different types of geometric features $g$ are mapped between the two domains via the homography matrix: $g\u0026rsquo;=f_1(H, g)$. If we are able to identify corresponding features in both domains, we could then try to find a way to revert this process and compute $H=f_2(g, g\u0026rsquo;)$.\n2.1. From points/lines Let us start by the simplest scenario where all equations are linear. For the sake of simplicity, we will focus on the retrieval of the homography from a set of point correspondences. However, notice there is a duality between points $\\Longleftrightarrow$ lines.\n$$ \\begin{equation} \\vec{p\u0026rsquo;}=H\\cdot\\vec{p} \\end{equation} $$\n$$ \\begin{equation} \\vec{l}=H^T\\cdot\\vec{l\u0026rsquo;} \\end{equation} $$\nAs a result, the process of retrieving the homography from a set of line correspondences would be completely analogous.\nExample of non-collinear pairs of points and non-concurrent pairs of lines that would allow to retrieve the homography matrix. 2.1.1. Problem formulation We can expand the equation for projecting a point in homogenous coordinates between two 2D planes:\n$$ \\begin{equation} s\\cdot \\begin{bmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ 1 \\\\ \\end{bmatrix}= \\begin{bmatrix} h_{11} \u0026amp; h_{12} \u0026amp; h_{13}\\\\ h_{21} \u0026amp; h_{22} \u0026amp; h_{23}\\\\ h_{31} \u0026amp; h_{32} \u0026amp; h_{33}\\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\end{equation} $$\nFor each homogenous coordinate, we get:\n$$ \\begin{equation} s\\cdot x\u0026rsquo;=h_{11}\\cdot x + h_{12}\\cdot y + h_{33} \\end{equation} $$\n$$ \\begin{equation} s\\cdot y\u0026rsquo;=h_{21}\\cdot x + h_{22}\\cdot y + h_{23} \\end{equation} $$\n$$ \\begin{equation} s=h_{31}\\cdot x + h_{32}\\cdot y + h_{33} \\end{equation} $$\nBy replacing $s$ we arrive at:\n$$ \\begin{equation} h_{31} \\cdot x\\cdot x\u0026rsquo; + h_{32}\\cdot y \\cdot x\u0026rsquo; + h_{33}\\cdot x\u0026rsquo;=h_{11}\\cdot x + h_{12}\\cdot y + h_{33} \\end{equation} $$\n$$ \\begin{equation} h_{31} \\cdot x\\cdot y\u0026rsquo; + h_{32}\\cdot y \\cdot y\u0026rsquo; + h_{33}\\cdot y\u0026rsquo;=h_{21}\\cdot x + h_{22}\\cdot y + h_{23} \\end{equation} $$\nWe can vectorise the homography matrix elements into\n$$ \\begin{equation} \\vec{h}=\\left[ h_{11}, h_{12}, h_{13}, h_{21}, h_{22}, h_{23}, h_{31}, h_{32}, h_{33}\\right]^T \\end{equation} $$\nwhich gives us the following homogenous system:\n$$ \\begin{equation} \\begin{bmatrix} x \u0026amp; y \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; -xx\u0026rsquo; \u0026amp; -yx\u0026rsquo;\u0026amp; -x\u0026rsquo;\\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; x \u0026amp; y \u0026amp; 1 \u0026amp; -xy\u0026rsquo; \u0026amp; -yy\u0026rsquo;\u0026amp; -y\u0026rsquo;\\ \\end{bmatrix} \\cdot \\vec{h} = 0 \\end{equation} $$\nWe can observe that a pair of points gives us two linear equations. Therefore, we could collect a set of point pairs and stack together all the equations to obtain a system of the form:\n$$ \\begin{equation} A\\cdot\\vec{h}=0 \\end{equation} $$\n2.1.2. Noise amplification: the horizon line Importantly, this system is not very robust to noise corruption, and the contribution to the accuracy of the solution depends on the point location. To understand why, let us focus on the scenario depicted below.\nExample of 4 point pair correspondences. We can observe that the upper sideline is relatively close to the horizon line. In the real world, the distance between its endpoints (red and cyan) is exactly the same as the distance for the bottom sideline endpoints (green and orange).\nHowever, due to the perspective transform, that distance is substantially smaller in the projected space. When we label the pixel coordinates in the left image, a slight error in the points for the top sideline will correspond to a much bigger error in yards than an equivalent pixel error for the bottom sideline.\nIn order to mitigate the impact of deviations, a common strategy consists of labelling more than the minimum number of required points (four). In this way, we make our system more robust to noise contamination.\n2.1.3. Solution: least squares estimator The goal is thus to solve the homogenous system of linear equations of the form\n$$ \\begin{equation} A\\cdot\\vec{h}=0 \\end{equation} $$\nRecall that the matrix H has 8 degrees of freedom (DoF). In order to solve this system, we therefore need 8 linearly independent equations. This can be achieved by having 4 different and non-colinear points (or 4 different non-concurrent lines, equivalently).\nIf we have more points, the system will be overdetermined. In general, that means there will not be an exact solution, so it seems reasonable to instead solve the Total Least Squares (TLS) problem given by\n$$ \\begin{equation} h^*=\\argmin_h \\lVert A\\cdot \\vec{h} \\rVert, \\qquad s.t. \\,\\,\\,\\,\\, \\lVert \\vec{h} \\rVert=1 \\end{equation} $$\nWe have added an extra constraint in order to avoid the trivial solution $\\vec{h}=\\vec{0}$. Bear in mind that $H$ is also only determined up to scale, so the unitary norm constraint is not restrictive at all (other constraints are also valid, such as $h_{33} = 1$, which is also a common choice).\nThis problem can easily be solved by resorting to the Singular Value Decomposition (SVD) of matrix A\n$$ \\begin{equation} A=U \\cdot\\Sigma \\cdot V^T \\end{equation} $$\nwhere $U$ and $V$ are unitary matrices defining an orthonormal basis for the column and row space of $A$ respectively. We can leverage this property for the following equalities\n$$ \\begin{equation} \\lVert A\\cdot\\vec{h} \\rVert=\\lVert U \\cdot\\Sigma \\cdot V^T\\cdot\\vec{h}\\rVert=\\lVert \\Sigma \\cdot V^T\\cdot\\vec{h}\\rVert \\end{equation} $$\n$$ \\begin{equation} \\lVert \\vec{h} \\rVert=\\lVert V^T\\cdot\\vec{h}\\rVert \\end{equation} $$\nAs a consequence, the system in $(13)$ is equivalent to\n$$ \\begin{equation} h^*=\\argmin_h \\lVert \\Sigma \\cdot V^T\\cdot\\vec{h}\\rVert, \\qquad s.t. \\,\\,\\,\\,\\, \\lVert V^T \\cdot \\vec{h} \\rVert=1 \\end{equation} $$\nand if we redefine $V^T\\cdot\\vec{h}=\\vec{g}$, our problem has simplified substantially to\n$$ \\begin{equation} \\argmin_g \\lVert \\Sigma \\cdot \\vec{g}\\rVert, \\qquad s.t. \\,\\,\\,\\,\\, \\lVert \\vec{g} \\rVert=1 \\end{equation} $$\nSince $\\Sigma$ is a diagonal matrix with its elements sorted in decreasing order\n$$ \\begin{equation} \\lVert \\Sigma \\cdot \\vec{g}\\rVert=\\sum_i (\\sigma_i\\cdot g_i)^2 \\end{equation} $$\nThis sum is minimized by setting\n$$ \\begin{equation} \\vec{g}=\\left[0, 0, \\,\\,\\,\\cdots\\,\\,\\,, 0, 1\\right] \\end{equation} $$\nFinally, recall that $\\vec{h}=V\\cdot\\vec{g}$, which implies that $\\vec{h}$ is just the eigenvector corresponding to the smallest eigenvalue.\nOn the other hand, if we just have four points or our observations are perfectly noiseless, the system $A\\cdot\\vec{h}=0$ has a unique solution. In that scenario, $A$ can be reduced to an $8x9$ matrix of rank 8. That means it will have a 1D null-space from which we can compute a non-trivial ($\\vec{h}\\neq \\vec{0})$ solution: the eigenvector corresponding to the null eigenvalue.\nTry it yourself!\nYou can see an example of how to retrieve the camera from a set of point/line correspondences in the repository. In order to do so, just install the package (poetry install) and then run\n# from points poetry run python -m projective_geometry homography-from-point-correspondences-demo # from lines poetry run python -m projective_geometry homography-from-point-correspondences-demo This will generate the following figure when retrieving the homography from a set of point correspondences:\nRetrieve homography from annotated points (red). Pitch template projection from retrieved homography is displayed in blue. And the following figure when retrieving the homography from a set of line correspondences:\nRetrieve homography from annotated lines (red). Pitch template projection from retrieved homography is displayed in blue. ‚ö†Ô∏è OpenCV provides a built-in method that retrieves the homography from a set of point correspondences: cv2.findHomography()\nYou can try it out in the repo for comparison by calling:\ncamera = Camera.from_point_correspondences_cv2( pts_source=points_template, pts_target=points_frame, ) 2.2. From conics Example of conic correspondences found in an hocker ice rink template. Similarly, we could try and estimate the homography from conic correspondences. We already know conics fulfill the following equation, up to a scale:\n$$ \\begin{equation} s\\cdot M\u0026rsquo;=H^{-T}\\cdot M\\cdot H^{-1} \\end{equation} $$\nwhere the scale factor satisfies\n$$ \\begin{equation} s^3\\cdot\\text{det}(M\u0026rsquo;)=\\frac{\\text{det}(M)}{\\text{det}(H)^2} \\end{equation} $$\nWe can freely force $\\text{det}(H)=1$, which implies:\n$$ \\begin{equation} s=\\sqrt[3]{\\frac{\\text{det}(M)}{\\text{det}(M\u0026rsquo;)}} \\end{equation} $$\nTherefore, without loss of generality, we can always normalize $M\u0026rsquo;=s\\cdot M$, so $\\text{det}(M)=\\text{det}(M\u0026rsquo;)$ and get rid of the scaling factor.\nUnder the assumption that the conics are non-degenerate ($\\text{det}(M)\\neq 0)$, we can combine this equation for two pair of corresponding ellipses ${(M_i, M_i\u0026rsquo;),\\,\\, (M_j, M_j\u0026rsquo;)}$\n$$ \\begin{equation} M_i^{-1}\\cdot M_j=H^{-1}\\cdot M_i\u0026rsquo;^{-1}\\cdot M_j\u0026rsquo;\\cdot H \\end{equation} $$\nor equivalently\n$$ \\begin{equation} H\\cdot M_i^{-1}\\cdot M_j-M_i\u0026rsquo;^{-1}\\cdot M_j\u0026rsquo;\\cdot H = 0 \\end{equation} $$\nThis forms a set of linear equations in the items forming the homography matrix $H$. Consequently, we can form a system of equations similar to the one we resolved earlier:\n$$ \\begin{equation} B_{ij}\\cdot\\vec{h}=0 \\end{equation} $$\nImportantly, two pairs of ellipses are not enough to uniequivocally determine the homography. We instead need at least 3 pairs of ellipses. By stacking the equations corresponding to each combination of two pairs of ellipses, we arrive at the final system\n$$ \\begin{equation} B\\cdot\\vec{h}=0 \\end{equation} $$\nthat we can solve following the procedure defined for the previous scenario.\nTry it yourself!\nYou can see an example of how to retrieve the camera from a set of ellipse correspondences in the repository. In order to do so, just install the package (poetry install) and then run\n# from ellipses poetry run python -m projective_geometry homography-from-ellipse-correspondences-demo This will generate the following figure:\nRetrieve homography from annotated ellipses (red). Pitch template projection from retrieved homography is displayed in blue. 2.3. From multiple features Example of different types of geometric features correspondences between a football broadcast frame and the pitch template. If we have identified a set of different matching geometric features (i.e.: points, lines, conics), one could solve the problem by trying to minimize a cost function that combines multiple terms. Ideally, one should be able to compute a convex metric that captures the similarity for each type of feature.\nExample of distances (marked in green) between pairs of (a) points, (b) lines and (c) ellipses. This way, finding the homography boils down to solving the following optimisation problem\n$$ \\begin{equation} \\vec{h^*} = \\argmin \\textcolor{blue}{\\mathcal{L_{\\text{p}}}(\\vec{p\u0026rsquo;}, H\\cdot\\vec{p})} + \\textcolor{red}{\\mathcal{L_{\\text{l}}}(\\vec{l\u0026rsquo;}, H^{-T}\\cdot\\vec{l})} + \\textcolor{green}{\\mathcal{L_{\\text{ell}}}(M\u0026rsquo;, H^{-T}\\cdot M\\cdot H^{-1})} \\end{equation} $$\nTry it yourself!\nYou can see an example of how to retrieve the camera from a set of point, line and ellipse correspondences in the repository. In order to do so, just install the package (poetry install) and then run\n# from ellipses poetry run python -m projective_geometry homography-from-correspondences-demo This will generate the following figure:\nRetrieve homography from annotated points (red), lines (green) and ellipses (yellow). Pitch template projection from retrieved homography is displayed in blue. 3. Via a pair images So far we have focused in retrieving the homography from a set of matching geometric features. What if instead we are given two photographs of the same scene taken from different angles?\nTwo photographs of the same scene taken from different angles. One way to proceed would be to identify and match geometric features, then apply the previously described procedures. The identification could be done manually, or automatically. The latter has been an active research area over recent decades, and multiple methods can be used, such as Harris corner detector, Canny edge detector, SIFT features, SURF features or ORB features. Interestingly, some of these detectors serve as descriptors too, which in turn provides a way to pair them out of the box.\nAlternatively, one could view the homography estimation as a registration problem. Let us say we have two image, the source $I(\\vec{x})$ and the target $T(\\vec{x})$, where $\\vec{x}=[x, y]^T$ corresponds to the pixel coordinates for the images. Moreover, let $W(\\vec{x}; \\vec{h}) = H\\cdot \\vec{p}$ be the transform that warps a set of pixels under the projective transform. $H$ would be the homography matrix characterizing the transform, and $\\vec{h}$ its vectorized form. Then we can minimize the following cost function:\n$$ \\begin{equation} \\sum_{\\vec{x}} |I(W(\\vec{x}; \\vec{h})) - T(\\vec{x}) |^2 \\end{equation} $$\nLucas-Kanade (LK) is a popular iterative algorithm widely used to tackle this kind of problem [6]. It relies on the computation of the gradient, under the assumption that objects may be located at different positions across the two images, but their appearance remains the same. There are multiple extensions, such as:\nHierarchical LK: resolves the problem at multiple resolutions to estimate iteratively gross-to-fine motion. Multimodal LK: for images where contrast varies (such as in between CT-scans and MRIs), but structure remains. Thus, the pixel intensity similarity is not a valid metric anymore. Instead, metrics based on entropy or sparisty can be used (see [7] for details). Given a set of geometric features identified in one of the images, LK algorithm can also be used directly to solve the matching step. However, it is important to rely on static features. If moving objects are used instead, that would corrupt the estimation of the camera motion.\nTry it yourself!\nYou can see an example of how to retrieve the camera from two images in the repository. In order to do so, just install the package (poetry install) and then run\n# from ellipses poetry run python -m projective_geometry homography-from-image-registration This will grab the following two images:\nTarget image we‚Äôre trying to align against. Source image we‚Äôre trying to align. It will try to find a set of matching keypoints in both, as illustrated below:\nSet of identified matching keypoints. And it will use those to retrieve the homography that allows to warp the source image (red border) onto the target one:\nSource image (red border) warped onto the target image with the retrieved homography.. 4. Via ML model Finally, a modern approach to tackle the homography estimation problem consists of using an ML trained model that is able to predict the projective transform given the input. If we‚Äôre trying to align two given images, those would serve as the input to the model. Alternatively, if we‚Äôre trying to map a given image to pre-defined template, one would only need to provide the former, since the template is static. For the sake of simplicity, we will focus on this second case.\na) Model that predicts the projective transform parametrisation that aligns two given images, b) Model that predicts the projective transform parametrisation that aligns a given image against a pre-defined static template. One feature that seems to have a substantial impact in the quality of the predictions is the parametrisation used to characterize the projective transform. The possible choices are:\nHomography matrix: the model would directly regress the parameters in the homography matrix. However, as pointed out in [8], those parameters consist of a mix of both rotational, translational and shearing terms. Therefore, they will likely vary in very different scales, so it would be necessary to balance their contribution to the loss.\nProjected geometric features: alternatively one could parametrize the homography by the projected coordinates of a set of pre-defined geometric features. In the simplest case, it would suffice to use 4 points, as proposed in [8, 9]. Alternatively, in order to increase the robustness in the prediction, one could create a grid of $N\\times M$points and predict the locations for all of them. Moreover, one could also predict other features such as lines, ellipses‚Ä¶ as done in [11]. From the two set of geometric features (the pre-defined static one, and its projected version predicted by the model), it would suffice to apply one of the procedures described before in order to retrieve the homography matrix. For the sake of the explanation, let us say we choose to predict the location of the projection for a set of points within a rectangular grid. Another important choice is therefore what domain to predict their location in. There are two choices here: Template domain: this forces the model to implicitly learn the mechanics of the projective transform, since it will have to learn their location after projecting the image grid to the template domain (see below). a) Static grid in image domain whose projection the model will have to learn, b) Projected grid predicted by the model for the given frame. The hope is that the model, by means of ingesting enough data, is able to learn these mechanics. However, it seems very inefficient to ask the model to learn this transform when its characterization is already known and mathematically described. Therefore, the following approach seems a much better way to tackle the problem. Image domain: this alleviates the complexity of the task at hand, since the model will only have to learn to identify the relevant keypoints in the given image. a) Static grid in template domain whose back-projection the model will have to learn, b) Back-projected grid predicted by the model for the given frame. As we have stated, by using a grid with more than the bare minimum required (4 points), we make the process more robust to noise corruption. However, this comes at a cost: the model is now free to predict the keypoints in a way that does not correspond to a projective deformation of a rectangular grid. Once again, the expectation is that, with enough data, the model will implicitly learn there is a relationship between the keypoints in the grid. But asking the model to learn that well-known relationship seems an inefficient usage of the available data. So there is a trade-off there between robustness and data efficiency. On the other hand, predicting the features location allows for another in choice. The problem can be posed as:\nRegression problem: in this case, one would simply train the model to predict the x-y coordinates of the geometric feature in question, resorting to any regression loss such as the l2 distance. Model is asked to find pixel (x-y) coordinates of selected template keypoint (marked in cyan) in the given image. Classification problem: alternatively, one could train the model to predict the probability of each pixel in the image corresponding to the seek-out keypoint, and then estimate its location by taking the one with the highest one. Model is asked to predict the probability (color-coded in grayscale) of a pixel in the given image to correspond to selected template keypoint (marked in cyan). Why bother to solve a regression problem via classification instead? Well, although this is not yet well-understood, there is empirical evidence pointing towards classification being more accurate at the regression task. The following Twitter thread provides some insights of why that might be the case.\n...\n\u0026mdash; Name (@username) Date One hypothesis revolves around the gradient computation during training. [12] argues that regression loss results in a smaller gradient the closer we get to the correct output. On the other hand, the gradient for the classification loss does not depend on how close we are to the underlying true value. This means that for bad predictions, the first loss is able to correct faster. However, once the prediction enters the small error regime, the classification approach would be more effective at narrowing the gap to the ground-truth. Another hypothesis suggests that when dealing with multi-modal data, whereas regression forces the model to make a choice, classification allows it to express uncertainty. In our case, the model could express that by assigning similar probability scores to multiple disjoint areas in the image. In such a way, the model may therefore be able to better capture the underlying multi-modal distribution of the input.\n5. Summary In this post we have shown multiple approaches that allow to retrieve the homography matrix. Those include:\nTechniques that rely on manual input, i.e. labelled geometric features such as keypoints, lines or ellipses. Techniques exploiting classical computer vision algorithms, such as Lukas-Kanade. Techniques based on Deep Learning models The latter constitute the state-of-the-art and is illustrated in the video below. It displays the pitch template (red) projected using the homography retrieved in a fully automated fashion.\nHomography demo Example of homography estimation in video clip. 6. References Richard Hartley and Andrew Zisserman (2000), Multiple View Geometry in Computer Vision, Cambridge University Press. Henri P. Gavin (2017), CEE 629 Lecture Notes. System Identification Duke University, Total Least Squares Richard Szeliski (2010), Computer Vision: Algorithms and Applications, Springer OpenCV Libary: Basic concepts of the homography explained with code Juho Kannala et. al. (2006), Algorithms for Computing a Planar Homography from Conics in Correspondence, Proceedings of the British Machine Vision Conference 2006. Simon Baker and Iain Matthews (2004), Lucas-Kanade 20 years on: A unifying framework. International Journal of Computer Vision. Lucilio Cordero Grande et. al. (2013), Groupwise Elastic Registration by a New Sparsity-Promoting Metric: Application to the Alignment of Cardiac Magnetic Resonance Perfusion Images, IEEE Transactions on Pattern Analysis and Machine Intelligence. Detone et.al. (2016), Deep Image Homography Estimation, arXiv. Ty Nguyen et. al. (2018), Unsupervised Deep Homography: A Fast and Robust Homography Estimation Model, arXiv. Wei Jiang et. al. (2019), Optimizing Through Learned Errors for Accurate Sports Field Registration, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV) Xiaohan Nie et. al. (2021), A Robust and Efficient Framework for Sports-Field Registration, 2021 IEEE Winter Conference on Applications of Computer Vision (WACV) James McCaffrey (2013), Why You Should Use Cross-Entropy Error Instead Of Classification Error Or Mean Squared Error For Neural Network Classifier Training ","permalink":"http://localhost:1313/posts/projective_geometry/estimating_homography_matrix/","summary":"Table of Contents 1. Introduction 2. Via geometric features 2.1. From points/lines 2.1.1. Problem formulation 2.1.2. Noise amplification: the horizon line 2.1.3. Solution: least squares estimator 2.2. From conics 2.3. From multiple features 3. Via a pair images 4. Via ML model 5. Summary 6. References 1. Introduction So far, we have figured out how to:\nMathematically characterize the transform between the 3D world and a 2D image of it Map different types of objects between the two domains However, one might wonder: how do we actually compute the homography matrix that fully describes that transform in the first place?","title":"üë®üèª‚Äçüíª Projective Geometry: Estimating the homography matrix"},{"content":" Table of Contents 1. Motivation: sports analytics 2. Points 3. Lines 4. Conics 4.1. Projection 4.2. Distortion: Objects behind the camera plane 5. Images 5.1. Projection via object decomposition 6. References 1. Motivation: sports analytics At this point, we know how to mathematically characterise the mapping between the 3D world and a 2D image capturing it. So it seems natural to wonder: what can we do with it? In this post, I will focus on a use case that I happen to be familiar with, but there are many others you can think of.\nImagine you work for a sports team, and the coach wants to monitor the performance of the players. Let us say he or she is interested in physical metrics such as the distance covered, average speed, peak speed, average location‚Ä¶ So you, being the data scientist in the organisation, are asked to find out those metrics, and the only source of data you are provided with is video footage from the games.\nIt is important to bear in mind what is preserved and what is not after a projective transform:\nConcurrency (multiple lines intersecting at a single point) is preserved Collinearity (multiple points belonging to the same line) is preserved Parallelism (lines equidistant to each other) is not preserved! The result of these properties manifests as spatial distortions, which in turn render it unfeasible to realiably measure distances in image space. Take the following frame from an NBA game for instance:\nExample of broadcast frame from an NBA game with labelled players. If we were able to retrieve the homography for this particular frame and locate the players on it, then we would be able to back project them and obtain their precise coordinates in the court template:\nExample of NBA court template. Furthermore, if we were able to do this process over all frames in the video, we would be able to track players throughout the whole game and measure any physical metric we care about.\nAuxiliary repository 2. Points In the previous post (Building the Homography Matrix from scratch), we already saw how to project a point from the 3D world into a 2D image\n$$ \\begin{equation} s\\cdot \\begin{bmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ 1 \\\\ \\end{bmatrix}= \\begin{bmatrix} h_{11} \u0026amp; h_{12} \u0026amp; h_{13} \u0026amp; h_{14}\\\\ h_{21} \u0026amp; h_{22} \u0026amp; h_{23} \u0026amp; h_{24}\\\\ h_{31} \u0026amp; h_{32} \u0026amp; h_{33} \u0026amp; h_{34}\\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\\\ \\end{bmatrix} \\end{equation} $$\nImportantly, this projection is not revertible. The reason is that each point in the 2D image is the result of collapsing a ray of points in the 3D world onto the image. In order to resolve the ambiguity of which one we are observing, multiple camera angles would be needed\nExample of multiple camera angles of the same NBA court template taken with two different cameras. Nevertheless, if we assume all objects of interest are located in a 2D plane, we end up with a 2D‚Üí2D mapping, which is revertible. In the scenario that motivated this post, that 2D plane in the 3D world would be the ground ($z=0$): we are interested in tracking player locations across the court, so it suffices to track their feet under the assumption that most of the time those are in the ground.\nUnder this circumstances, our projection simplifies to\n$$ \\begin{equation} s\\cdot \\begin{bmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ 1 \\\\ \\end{bmatrix}= \\begin{bmatrix} h_{11} \u0026amp; h_{12} \u0026amp; h_{13} \u0026amp; h_{14}\\\\ h_{21} \u0026amp; h_{22} \u0026amp; h_{23} \u0026amp; h_{24}\\\\ h_{31} \u0026amp; h_{32} \u0026amp; h_{33} \u0026amp; h_{34}\\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\\\ 0 \\\\ 1 \\\\ \\end{bmatrix} \\end{equation} $$\nor equivalently\n$$ \\begin{equation} s\\cdot \\begin{bmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ 1 \\\\ \\end{bmatrix}= \\begin{bmatrix} h_{11} \u0026amp; h_{12} \u0026amp; h_{14}\\\\ h_{21} \u0026amp; h_{22} \u0026amp; h_{24}\\\\ h_{31} \u0026amp; h_{32} \u0026amp; h_{34}\\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\end{equation} $$\nSo in order to project a point, we just need to convert it to homogenous coordinates and multiply with the homography matrix:\n$$ \\begin{equation} \\vec{p\u0026rsquo;}=H\\cdot\\vec{p} \\end{equation} $$\nThis will give us the projected point in homogenous coordinates. In order to retrieve the pixel location within the image, we just need to divide by the scaling factor ($s$):\n$$ \\begin{equation} \\vec{p\u0026rsquo;}=s\\cdot \\begin{bmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ 1 \\\\ \\end{bmatrix}\\Longrightarrow x\u0026rsquo;=\\frac{\\vec{P}[0]}{\\vec{P}[2]}, \\:\\:\\: y\u0026rsquo;=\\frac{\\vec{P}[1]}{\\vec{P}[2]} \\end{equation} $$\nExample\ncamera = Camera(H=np.array([ [ 8.69135802e+00, -2.96296296e+00, 6.40000000e+02], [ 0.00000000e+00, 7.33333333e+00, 2.93333333e+02], [ 0.00000000e+00, -4.62962963e-03, 1.00000000e+00], ])) pt = Point(x=0, y=0) projected_pt = project_points(camera=camera, pts=(pt, )) print(projected_pt[0]) \u0026gt;\u0026gt;\u0026gt; Point(x=640.0, y=293.333333) 3. Lines A straight line is fully described in 2D through its vector representation $\\vec{l}$\n$$ \\begin{equation} \\vec{l}= \\begin{bmatrix} a \\\\ b \\\\ c \\\\ \\end{bmatrix} \\end{equation} $$\nA point expressed in homogeneous coordinates, $\\vec{p}=[x, y]^T$, belongs to the line if it satisfies its general form\n$$ \\begin{equation} a\\cdot x + b\\cdot y + c = 0 \\end{equation} $$\nThat is to say, it is orthogonal to the line vector\n$$ \\begin{equation} \\vec{l}^T\\cdot \\vec{p} = 0 \\end{equation} $$\nWe can leverage the basic property of collinearity preservation under projective geometry: if a point $\\vec{p}$ lies in a line $\\vec{l}$, the projected point $\\vec{p\u0026rsquo;}=H\\cdot\\vec{p}$ will lie in the projected line $\\vec{l\u0026rsquo;}$. Therefore, it must satisfy\n$$ \\begin{equation} \\vec{l\u0026rsquo;}^T\\cdot \\vec{p\u0026rsquo;} = 0 \\end{equation} $$\nor equivalently\n$$ \\begin{equation} \\vec{l\u0026rsquo;}^T\\cdot H \\cdot \\vec{p} = 0 \\end{equation} $$\nSince both equations are null, they must be equal\n$$ \\begin{equation} \\vec{l}^T\\cdot\\vec{p\u0026rsquo;} = \\vec{l\u0026rsquo;}^T\\cdot H\\cdot\\vec{p} \\end{equation} $$\nwhich necessarily implies\n$$ \\begin{equation} \\vec{l}^T = \\vec{l\u0026rsquo;}^T\\cdot H \\end{equation} $$\nConsequently, in order to project a line, we simply need to apply\n$$ \\begin{equation} \\vec{l\u0026rsquo;} = H^{-T}\\cdot \\vec{l} \\end{equation} $$\nExample\ncamera = Camera(H=np.array([ [ 8.69135802e+00, -2.96296296e+00, 6.40000000e+02], [ 0.00000000e+00, 7.33333333e+00, 2.93333333e+02], [ 0.00000000e+00, -4.62962963e-03, 1.00000000e+00], ])) ln = Line(a=0, b=1, c=0) projected_ln = project_lines(camera=camera, lines=(ln, )) print(projected_ln[0]) \u0026gt;\u0026gt;\u0026gt; Line(a=0.0, b=7.33333333, c=293.333333) 4. Conics Conic curves receive their name since they can be obtained fr the intersection between a plane and a cone with two nappes:\nCircle: plane is perpendicular to the cone axis Ellipse: plane intersects with only one half of the plane forming a closed surface Hyperbola: plane intersects with both halves of the plane Parabola: plane is parallel to the cone slant Illustration of different conic formation for the four different types. From left to right: parabola, circle, ellipse and hyperbola. 4.1. Projection Mathematically, a conic curve can be modelled by means of its matrix representation $M$:\n$$ \\begin{equation} M= \\begin{bmatrix} A \u0026amp; B/2 \u0026amp; D/2 \\\\ B/2 \u0026amp; C \u0026amp; E/2 \\\\ D/2 \u0026amp; E/2 \u0026amp; F \\\\ \\end{bmatrix} \\end{equation} $$\nAny point that belongs to it, expressed in homogeneous coordinates, $\\vec{p}=[x, y, 1]^T$, must satisfy the following second-degree polynomial equation:\n$$ \\begin{equation} A\\cdot x^2 + B\\cdot x\\cdot y + C\\cdot y^2 + D \\cdot x + E \\cdot y + F = 0 \\end{equation} $$\nor in matrix form:\n$$ \\begin{equation} \\vec{p}^T\\cdot M \\cdot \\vec{p} = 0 \\end{equation} $$\nOnce again, we can take advantage of the collinearity preservation property, which is valid for any kind of line, not just straight one. Let us say that a point $\\vec{p}$ lies in the ellipse given by $M$. Then the projected point $\\vec{p\u0026rsquo;}=H\\cdot\\vec{p}$ must lie in the projected ellipse given by $M'$\n$$ \\begin{equation} \\vec{p\u0026rsquo;}^T\\cdot M\u0026rsquo; \\cdot \\vec{p\u0026rsquo;} = 0 \\end{equation} $$\nReplacing the projected point we obtain\n$$ \\begin{equation} (H\\cdot\\vec{p})^T\\cdot M\u0026rsquo; \\cdot H\\cdot\\vec{p} = 0 \\end{equation} $$\nNotice the equivalence of the previous null equations\n$$ \\begin{equation} \\vec{p}^T\\cdot H^T\\cdot M\u0026rsquo; \\cdot H\\cdot\\vec{p} = \\vec{p}^T\\cdot M \\cdot \\vec{p} \\end{equation} $$\nwhich necessarily implies\n$$ \\begin{equation} H^T\\cdot M\u0026rsquo; \\cdot H = M \\end{equation} $$\nTherefore, projecting a conic reduces to\n$$ \\begin{equation} M\u0026rsquo; = H^{-T}\\cdot M \\cdot H^{-1} \\end{equation} $$\n4.2. Distortion: Objects behind the camera plane Importantly, although the projective transform maps a conic to another conic, it does not necessarily preserve its type. It is not hard to visualise how a circle can become an ellipse depending on the perspective. However, it is not intuitive at all how it might turn into a parabola/hyperbola. So what is going on?\nThe key feature is the camera plane. It corresponds to the 2D plane parallel to the camera film (where the image is projected onto) that passes through the camera pinhole. Objects in front of it are projected to the film plane (the infinite extension of the film with finite dimensions), but are only captured in the image whenever they lie within the covered pyramid, as displayed below:\nIllustration of the camera plane, given by a 2D plane parallel to the camera film passing through the pinhole. On the other hands, objects that lie right on the camera plane would never be projected onto the image plane, since the ray that goes both through them and the pinhole is parallel to the image plane:\nIllustration of an object located at the camera plane, which would never be projected to the image unless its located exactly at the camera pinhole. Finally, when an object is behind the camera plane, the projective transform is still able to project it onto the image film.\nIllustration of an object located behind the camera plane, which would never be projected to the image in the real world. However, as a result of a mathematical artefact, it would end up in the image according to the projection equations. However, this is a mathematical artefact!!! Whenever we take a picture, we ca only capture objects within the field of view in front of the camera. Nonetheless, the mathematical transform that models this process can still be applied to objects behind the camera. Therefore, one must be careful and ensure this transform is restricted to objects in front of the camera, or we‚Äôd introduced unexpected distortions.\nLet us now go back to the case of conics. In order to better gain some insight, let us picture the following scenario: a couple of friends are playing frisbee and there happens to be a drone recording the scene from above\nScene depicting two people throwing a frisbee to each other with a camera drone filming the pass from above. To further simplify the scenario, let us assume the frisbee, of radius $r$, is the only object the camera is able to track. Furthermore, it is following a perfectly horizontal and straight trajectory in a plane parallel to the camera axis, starting at a distance $d$. The camera has focal length $f$ and projects onto an image of size $(W, H)$. Finally, let us define the origin of coordinates in our 3D world at the intersection between the 2D plane the frisbee travels through and the vertical line passing through the camera pinhole, located at a height $h$.\nDetailed depiction of the image capturing. The frisbee, represented as a 2D circle, follows a horizontal trayectory from right to left. The camera in turn pointing horizontally to the right and located above the frisbee trajectory. The following video displays what we would get if we were to mathematically compute the projection of the conic and display it on the image film. For clarity, we provide at the bottom an amplified view of the virtual image (the result of flipping horizontally and vertically the film image):\nDetailed depiction of the image captured by the camera as the frisbee follows its trajectory. Despite being a circle in the real world, it is captured as an ellipse, a parabola or a hyperbola depending on its position w.r.t. the camera plane. Whenever the frisbee is right at the edge of the camera plane, the frisbee gets mapped to a parabola. The reason is that point at the edge gets mapped to infinity, turning the circular frisbee into an open surface.\nMoreover, whenever the frisbee is partially in front of the camera, and partially behind it, there is a split in the projection. As a result, we get the unexpected hyperbola. Again, this is a mathematical artefact that would never manifest in a real photograph, it only arises if one naively projects without considering the location of objects w.r.t. the camera plane.\nNote: you can generate the previous video by simply running this script. In order to do so, just install the repository (poetry install) and then run\npoetry run python -m projective_geometry frisbee-demo --output \u0026lt;LOCATION_OUTPUT_VIDEO.mp4\u0026gt; 5. Images In order to project an image, we start by building a $(3, N)$ matrix $P=[\\vec{X}\\,\\,\\,\\vec{Y} \\,\\,\\,\\vec{1} ]^T$ with the homogenous coordinates for all pixels in the image grid:\nDepiction of the pixel grid in a soccer broadcast frame. Then we project the grid of points using the homography matrix characterising the camera and convert the resulting homogeneous coordinates to pixel locations\n$$ \\begin{equation} P\u0026rsquo; = H\\cdot P \\end{equation} $$\n$$ \\begin{equation} p_i\u0026rsquo;\u0026rsquo;=\\left[\\frac{p_i\u0026rsquo;[0]}{p_i\u0026rsquo;[2]}, \\frac{p_i\u0026rsquo;[1]}{p_i\u0026rsquo;[2]}\\right] \\,\\,\\,\\,\\,\\,\\,\\,\\forall p_i\u0026rsquo;\\in P' \\end{equation} $$\nDeformed grid as a result of a projective transform applied to the captured image. Finally, we just need to interpolate the image values at the projected grid\nDeformed image adjusted to the projected grid. OpenCV provides a method that applies this transform, cv2.warpPerspective. It provides different methods to carry out the interpolation, some of which are differentiable. Differentiability becomes essential for dealing with optimization problems. Most often, this kind of problems are tackled via gradient-descent algorithms. Therefore, whenever the image projection transform is part of the cost function we are trying to optimize for, one must ensure the underlying interpolation is carried out in a differentiable fashion.\nThis is useful for validating if a camera is properly corrected by visual inspection. You can simply project the source image using the camera and validate it overlaps with the target image. There is an example in case you want to try it out by yourself here, which you can run with the command below\npoetry run python -m projective_geometry celtics-demo --output \u0026lt;LOCATION_OUTPUT_IMAGE.png\u0026gt; This will display the projected court template on top of the given frame\nNBA broadcast frame with the court template projected overlayed in blue. It is the result of applying the projective transform according to the homography that characterizes the camera with with the frame was captured. 5.1. Projection via object decomposition Notice that when projecting a line, its width varies depending on where it lands on the image. This is expected behaviour and reflects how we perceive the 3D world: objects closer to us occupy a wider field of view than those further away.\nHowever, in some scenarios, one might be interested in having full control over the visualisation of projected objects. Whenever it is feasible to decompose the original image in its core geometric features, one could therefore project individually each feature and display it on the target image with the desired visualisation properties. For instance, the basketball court ban be broken down into all the line segments, circles and arcs, as illustrated below\nNBA court template displaying all geometric features (points, lines and conics) it consists of. The drawback, as already explained, is that objects might be partially or completely behind the camera plane. Consequently, if following this approach, it is essential to exclude those objects from the visualisation in order to avoid undesired artefacts.\n6. References Richard Hartley and Andrew Zisserman (2000), Page 143,¬†Multiple View Geometry in Computer Vision, Cambridge University Press. Wikipedia.¬†Conic sections and Matrix Representation Wei Jiang et. al. (2019), arXiv. Optimizing Through Learned Errors for Accurate Sports Field Registration OpenCV Library ","permalink":"http://localhost:1313/posts/projective_geometry/projecting_between_domains/","summary":"Table of Contents 1. Motivation: sports analytics 2. Points 3. Lines 4. Conics 4.1. Projection 4.2. Distortion: Objects behind the camera plane 5. Images 5.1. Projection via object decomposition 6. References 1. Motivation: sports analytics At this point, we know how to mathematically characterise the mapping between the 3D world and a 2D image capturing it. So it seems natural to wonder: what can we do with it? In this post, I will focus on a use case that I happen to be familiar with, but there are many others you can think of.","title":"üìê Projective Geometry: Projecting between domains"},{"content":" Table of Contents 1. Pinhole camera model 2. Intrinsic matrix 2.1. Setup 2.2. Homogeneous coordinates 2.3. Accounting for distortions 2.3.1. Digital images 2.3.2. Rephotographing Images 3. Extrinsic matrix 4. Homography matrix 5. References 1. Pinhole camera model When we capture something on camera, there is an interesting phenomenon going on: compression. We are taking a photograph of a 3D world, and capturing it in a 2D image. This 3D‚Üí2D space mapping inevitably leads to information loss. There are multiple locations in the 3D world that project exactly to the same position in the 2D image. Nonetheless, this observation should not come as a surprise to anyone: it is precisely the reason occlusions take place. Multiple objects at different places end up in the same location when viewed through a 2D projection, be it a film or our retina.\nSo how do we model that 3D‚Üí2D mapping? That is what projective geometry is all about. To understand this transform, let us start with the ideal pinhole camera model, illustrated below:\nExample of an analogic camera (cube at the bottom) pointing upwards to capture an ovni flying above it. In this setup, the camera consists of a simple dark box with a small aperture (termed pinhole), located in its front, and a film on its rear wall, with dimensions (W, H). The distance between the pinhole and the film is known as the focal length (f).\nThe image is formed when the light rays enter it through the pinhole (which we will assume of infinitesimal radius) and are captured by the photographic material the film is composed of. This way, an inverted image is obtained, which can be flipped in a post-processing step. Moreover, this is equivalent to forming a virtual image in front of the camera, at a distance f from the pinhole.\nExample of virtual inverted image formation. The light rays project to the film located at the back of the analogic camera, causing an inversion. In order to undo the inversion, we can flip the formed image, which is equivalent to capturing a virtual image in a plane in front of the film. 2. Intrinsic matrix 2.1. Setup In order to characterize the projection, let us start by defining the two system of coordinates involved in the transform:\n3D World: we define the origin of coordinates located at the camera pinhole. Furthermore, the cartesian axes will also be aligned with the camera axes. 2D Image: we define the origin of coordinates located at the bottom-left corner of the virtual image. Illustration of the two different coordinates systems: the world coordinate system, centered at the camera pinhole (pink) and the image coordinate system, centered at the bottom left corner of the virtual image (red). Given these arbitrary definitions, we can now determine where a point $p=(x, y, z)$ will be projected in the image. To do so, we just need to recall that the projection $P=(X, Y)$ is found at the intersection between the plane containing the virtual image, and the ray passing through both the point p and the pinhole $o=(0, 0, 0)$.\nFor simplicity, let us focus on how the projection takes place for the horizontal dimension $x$:\nLeft image shows how a a 3D point $\\vec{p}$ is projected into the virtual image at pixel $\\vec{P}$. Right image focuses on the ZX plane. We can observe that there are two similar triangles (black and green) on the right image. Therefore, the rate between the lengths of their sides is preserved:\n$$ \\begin{equation}\\frac{z}{x} = \\frac{f}{X-\\frac{W}{2}}\\end{equation} $$\nor equivalently\n$$ \\begin{equation}f\\cdot x+\\frac{W}{2}\\cdot z=X \\cdot z\\end{equation} $$\nWe can derive the projection along the y dimension in an analogous fashion, resulting in:\n$$ \\begin{equation}f\\cdot y+\\frac{H}{2}\\cdot z=Y \\cdot z\\end{equation} $$\n2.2. Homogeneous coordinates At this point, it is worth wondering: can we express the previous equations as a matrix product?\n$$ \\begin{equation} \\begin{bmatrix} f \u0026amp; 0 \u0026amp; \\frac{W}{2}\\\\ 0 \u0026amp; f \u0026amp; \\frac{H}{2}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} X\\cdot z \\\\ Y\\cdot z \\\\ z \\end{bmatrix} \\end{equation} $$\nIt is almost what we want, but we have an extra scaling factor on the right hand-side\n$$ \\begin{equation} \\begin{bmatrix} f \u0026amp; 0 \u0026amp; \\frac{W}{2}\\\\ 0 \u0026amp; f \u0026amp; \\frac{H}{2}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = z\\cdot \\begin{bmatrix} X \\\\ Y \\\\ 1 \\end{bmatrix} \\end{equation} $$\nAnd here is where homogeneous coordinates show up. For a point $\\vec{P}=[X,Y]^T$, its homogeneous coordinates can be defined as\n$$ \\begin{equation} \\vec{P_H}=\\begin{bmatrix} s\\cdot X \\\\ s\\cdot Y \\\\ s \\end{bmatrix} = s\\cdot \\begin{bmatrix} X \\\\ Y \\\\ 1 \\end{bmatrix} \\end{equation} $$\nLeveraging this definition, we can express the projection as\n$$ \\begin{equation}K\\cdot \\vec{p}=\\vec{P}_H \\end{equation} $$\nwhere $K$ is defined as the intrinsic matrix\n$$ \\begin{equation} K=\\begin{bmatrix} f \u0026amp; 0 \u0026amp; \\frac{W}{2}\\\\ 0 \u0026amp; f \u0026amp; \\frac{H}{2}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\end{equation} $$\nand $\\vec{p}$ corresponds to the 3D world point\n$$ \\begin{equation} \\vec{p}=\\begin{bmatrix} x\\\\ y\\\\ z \\end{bmatrix} \\end{equation} $$\nImportantIy, homogenous coordinates are defined up to a scale. Why is that the case? Going back to the introduction, we stated that there was a compression involved in projective geometry. We are capturing a 3D world into a 2D image. That implies that there is an infinite set of aligned points (i.e. a line) in the 3D world that are mapped exactly to the same location in image space. Take a look at the image below\nIllustration of the compression that occurs when projecting points from the 3D world into a 2D image. All points that belong to a ray going through the pinhole, such as $\\vec{p}$ and $\\vec{p'}$ are mapped to the same pixel. Once again, due to the similarity of the triangles involved, we know that\n$$ \\begin{equation}\\frac{z}{z\u0026rsquo;} = \\frac{x}{x\u0026rsquo;} = \\frac{y}{y\u0026rsquo;} = s \\end{equation} $$\nIf we plug in our matrix equation both $\\vec{p}$ and $\\vec{p\u0026rsquo;}$, we will obtain\n$$ \\begin{equation} \\vec{P_H}=\\begin{bmatrix} f\\cdot x + \\frac{W}{2}\\cdot z \\\\ f\\cdot y + \\frac{H}{2}\\cdot z \\\\ z \\end{bmatrix} \\end{equation} $$\nand\n$$ \\begin{equation} \\vec{P_H}\u0026rsquo;=\\begin{bmatrix} f\\cdot x\u0026rsquo; + \\frac{W}{2}\\cdot z\u0026rsquo; \\\\ f\\cdot y\u0026rsquo; + \\frac{H}{2}\\cdot z\u0026rsquo; \\\\ z\u0026rsquo; \\end{bmatrix} \\end{equation} $$\nIt is clear these two vectors are not the same. However, they are proportional\n$$ \\begin{equation} \\vec{P_H}\u0026rsquo;=s\\cdot\\vec{P_H} \\end{equation} $$\nConsequently, in homogeneous coordinates, they represent exactly the same point in image space! And how do we obtain its image coordinates $\\vec{P}=[X,Y]^T$? Following the definition in eq.(6), we simply need to divide by the scale factor encoded in the last dimension:\n$$ \\begin{equation} X=\\frac{{P_H}_x}{{P_H}_z} \\end{equation} $$\n$$ \\begin{equation} Y=\\frac{{P_H}_y}{{P_H}_z} \\end{equation} $$\n2.3. Accounting for distortions 2.3.1. Digital images So far, we have assumed we had an analog camera, so the image coordinates lived in a continuous space. However, most often we deal with digital images. Since the information needs to be stored in bits, we need to both discretize the locations at which we sample the image, and quantitize the values we measure.\nIllustration of the effects of sampling along the temporal dimension and quantizing the measured signal amplitud. Since we are focused on the spatial mapping, the effect we care about is the discretization that takes place in the projected image. If we use squared pixels, we will get something as displayed below:\nDepiction of the effects of capturing a real world object (left) as a 2D digital image, which only allows to store a discrete number of values at a discrete set of 2D pixel locations. The length of the pixel side is given by the width (height) of the image divided by the number of pixels along the corresponding dimension $N_x$ ($N_y$):\n$$ \\begin{equation} \\Delta=\\frac{W}{N_x}=\\frac{H}{N_y} \\end{equation} $$\nWe can now express the image coordinates using the pixel length as its units. That is to say, both the image dimensions and the focal length can be defined in pixels:\n$$ \\begin{equation} f=f_I\\cdot\\Delta,\\;\\;\\;\\;\\;\\;\\;\\; W=W_I\\cdot\\Delta,\\;\\;\\;\\;\\;\\;\\;\\; H=H_I\\cdot\\Delta \\end{equation} $$\nAs a result, if we want to obtain the projected coordinates in this discretized image space, we simply need to tweak slightly our intrinsic matrix:\n$$ \\begin{equation} K=\\begin{bmatrix} \\frac{f}{\\Delta} \u0026amp; 0 \u0026amp; \\frac{W}{2\\Delta}\\\\ 0 \u0026amp; \\frac{f}{\\Delta} \u0026amp; \\frac{H}{2\\Delta}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} = \\begin{bmatrix} f_I \u0026amp; 0 \u0026amp; \\frac{W_I}{2}\\\\ 0 \u0026amp; f_I \u0026amp; \\frac{H_I}{2}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\end{equation} $$\nSo what happens if instead we have non-squared pixels?\n$$ \\begin{equation} \\Delta_x = r\\cdot\\Delta_y \\end{equation} $$\nEffect of using rectangular (non-squared) pixels for the 2D digital image that captures the real world object. Well, that is how we end up with two different focal lengths along each dimension:\n$$ \\begin{equation} W=W_I\\cdot\\Delta_x \\end{equation} $$\n$$ \\begin{equation} H=H_I\\cdot\\Delta_y \\end{equation} $$\n$$ \\begin{equation} f=f{_I}_x\\cdot\\Delta_x \\end{equation} $$\n$$ \\begin{equation} f=f{_I}_y\\cdot\\Delta_y \\end{equation} $$\nwhich leads to the common definition of the intrinsic matrix (we have dropped the subindex $I$ for ease of notation)\n$$ \\begin{equation} K=\\begin{bmatrix} f_x \u0026amp; 0 \u0026amp; \\frac{W}{2}\\\\ 0 \u0026amp; f_y \u0026amp; \\frac{H}{2}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\end{equation} $$\n2.3.2. Rephotographing Images It is common to see an additional parameter in the intrinsic matrix: the skew factor. It accounts for a shearing effect, i.e., image axes not being perpendicular, which results in non-rectangular parallelogrammatic pixels.\nEffect of shearing distortion when capturing a real world object in a 2D digital image. Mathematically, it can be modelled by a simple change of basis. Therefore, we just need to find the coefficients of the point expressed in the new basis. Using trigonometry, we can derive.\nDetailed depiction of the trigonometry involved in the shearing distortion that maps a squared pixel (pink) to a rhomboid one (green). Using trigonometry, we can derive\n$$ \\begin{equation} \\cos(90-\\varphi)=\\sin(\\varphi)=\\frac{y}{y\u0026rsquo;} \\; \\; \\;\\rightarrow \\; y\u0026rsquo;=\\frac{y}{\\sin(\\varphi)} \\end{equation} $$\n$$ \\begin{equation} \\sin(90-\\varphi)=\\cos(\\varphi)=\\frac{x\u0026rsquo;-x}{y\u0026rsquo;} \\; \\; \\;\\rightarrow \\; x\u0026rsquo; = x - y\u0026rsquo;\\cdot \\cos(\\varphi) \\end{equation} $$\nand combining both equations\n$$ \\begin{equation} x\u0026rsquo; = x - y\\cdot\\cos(\\varphi) / \\sin(\\varphi) = x - y\\cdot\\cot(\\varphi) \\end{equation} $$\nor in matrix form\n$$ \\begin{equation}\\begin{bmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\end{bmatrix} =\\begin{bmatrix} 1 \u0026amp; -\\cot(\\varphi) \\\\ 0 \u0026amp; \\frac{1}{\\sin(\\varphi)} \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\end{equation} $$\nOverall, there is a nice way to decouple the different transforms we have discussed so far:\nScaling: projecting 3D world into 2D image\n$$ \\begin{equation} \\begin{bmatrix} f_x \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; f_y \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\end{equation} $$\nShearing: accounting for non-perpendicular axes\n$$ \\begin{equation} \\begin{bmatrix} 1 \u0026amp; -\\cot(\\varphi) \u0026amp; 0\\\\ 0 \u0026amp; \\frac{1}{\\sin(\\varphi)} \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\end{equation} $$\nShift: to move the origin of the image space to the bottom-left of the image.\n$$ \\begin{equation}\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; \\frac{W}{2}\\\\ 0 \u0026amp; 1 \u0026amp; \\frac{H}{2}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\end{equation} $$\nwhich overall results in the following intrinsic matrix\n$$ \\begin{equation}K\u0026rsquo; =\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; \\frac{W}{2}\\\\ 0 \u0026amp; 1 \u0026amp; \\frac{H}{2}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \u0026amp; -\\cot(\\varphi) \u0026amp; 0\\\\ 0 \u0026amp; \\frac{1}{\\sin(\\varphi)} \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} f_x \u0026amp; 0 \u0026amp; 1\\\\ 0 \u0026amp; f_y \u0026amp; 1\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\end{equation} $$\nIt can be reduced to\n$$ \\begin{equation} K\u0026rsquo;=\\begin{bmatrix} f_x \u0026amp; -f_x\\cdot\\cot(\\varphi) \u0026amp; \\frac{W}{2}\\\\ 0 \u0026amp; \\frac{f_y}{\\sin(\\varphi)} \u0026amp; \\frac{H}{2}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\end{equation} $$\nor how it is more often presented:\n$$ \\begin{equation} K\u0026rsquo;=\\begin{bmatrix} f_x \u0026amp; s \u0026amp; \\frac{W}{2}\\\\ 0 \u0026amp; f_y \u0026amp; \\frac{H}{2}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\end{equation} $$\nNotice how it is fully characterised with 5 parameters: the focal length along both dimensions $(f_x, f_y)$, the skew factor $s$ and the image size $(W, H)$.\nIt is worth pointing out that in most cases, the shearing effect is not present. It is very unlikely that a camera is flawed to the point of not having its axes perpendicular. However, it can arise when a picture is retaken and the film of the second camera is not parallel with the image.\n3. Extrinsic matrix So far, we have assumed our arbitrarily world coordinate system (pink) is perfectly aligned with our camera coordinate system (green), with its origin at the camera pinhole and its Cartesian axes parallel to the camera axes.\nIllustration of a shift and rotation of the camera w.r.t. the world coordinate system (pink). It allows us to define an additional camera coordinate system (green) centered at the camera pinhola and aligned with the camera plane. However, when we take photos of videos in the world, we often rotate and/or shift the camera. This results in a camera system that can be derived from the world system by applying two steps sequentially:\nRotate the axes by:\n$$ R(\\varphi_x,\\varphi_y,\\varphi_z)\\equiv R $$\nShift the origin by\n$$ t\\vec{}=\\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} $$\nThese two parameters $(R, \\vec{t})$ define the camera pose.\nIn such a scenario, it might be convenient to have a static coordinate system as a reference, which the camera does not provide anymore. Consequently, it is worth considering: how can we define mathematically the camera projection under these circumstances?\nIn order to account for this misalignment, it is useful to understand how we can map the coordinates between both coordinate systems. To better understand how that is done, let us simplify to 2D, as depicted in the following figure:\nExample of two different coordinate systems, pink one being rotated and shifted w.r.t. to pink one. A point can be expressed in both of them as $\\vec{p'}$ and $\\vec{p}$ respectively. The origin of coordinates in the camera system is located at $\\vec{t}=[t_x,t_y]$ w.r.t. the world system. Following this observation, we can infer that in order to align both origin of coordinates $O$ and $O\u0026rsquo;$, we just need to apply a shift given by the vector $\\vec{t}$:\nBy shifting the camera coordinate system by the vector $\\vec{t}$ we can center both system of coordinate systems at the same locatiton. However, we still need to account for the rotation between the axes, given by angle $\\varphi$.\nBy further rotating the camera coordinate system by angle $\\varphi$ we can also align both system of coordinate systems. As we can observe, now both systems are perfectly aligned (notice how we used the equal symbol between $p_3$ and $p\u0026rsquo;$, not just the equivalent symbol). Accordingly, to express a point in the camera system, we just need to apply two steps:\nReverse shift:\n$$ \\begin{equation} \\vec{p}_2=\\vec{p}-\\vec{t} \\end{equation} $$\nReverse rotation:\n$$ \\begin{equation} \\vec{p\u0026rsquo;}=\\vec{p}_3= R^{-1}\\cdot\\vec{p}_2= R^T\\cdot(\\vec{p}-\\vec{t}) \\end{equation} $$\nwhere we have exploited the fact that rotation matrices are unitary, so their inverse is equal to their transpose.\nIt is worth remarking that order matters!. If we were to rotate first, we would not be rotating around $O\u0026rsquo;$ as we would hope to in order to align the camera axes, but around $O$, as illustrated below\nIllustration of why order matters. If we apply first the rotation, and then the shift, both coordinates sistems end up with their axes parallel to each other, but their origin of coordinates misaligned. At this point, it is worth wondering: is there any way we can synthetize the previous equation into a simple matrix product?\n$$ \\begin{equation} \\vec{p\u0026rsquo;}= \\begin{bmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ z\u0026rsquo; \\end{bmatrix} = R^T\\cdot \\left( \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} - \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\right) \\end{equation} $$\nIt should not come as a surprise to see homogeneous coordinates come to the rescue once again:\n$$ \\begin{equation} \\vec{p_H\u0026rsquo;}=\\begin{bmatrix} x\u0026rsquo; \\\\ y\u0026rsquo; \\\\ z\u0026rsquo; \\\\ 1 \\end{bmatrix} = \\left[\\; R^T\\;\\; | \\;\\;-R^T\\cdot\\vec{t}\\;\\;\\right ]\\cdot \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} =\\left[\\; R^T\\;\\; | \\;\\;-R^T\\cdot\\vec{t}\\;\\;\\right ]\\cdot {p_H} \\end{equation} $$\nwhich allows us to define the extrinsic matrix as\n$$ \\begin{equation} E= \\left[\\; R^T\\;\\; | \\;\\;-R^T\\cdot\\vec{t}\\;\\;\\right ] \\end{equation} $$\nIn some texts, you might find an equivalent definition given by\n$$ \\begin{equation} E= \\left[\\; R\u0026rsquo;\\;\\; | \\;\\;\\vec{T}\\;\\;\\right ] \\end{equation} $$\nwhere $R\u0026rsquo;=R^T$ and $\\vec{T}=-R^T\\cdot\\vec{t}$.\nTo sum up, the extrinsic matrix is fully described by 6 parameters: the 3D location of the camera $(t_x, t_y, t_z)$ and its 3 rotation angles $(\\varphi_x,\\varphi_y,\\varphi_z)$ defining its orientation.\n4. Homography matrix In the light of these definitions, we have all the ingredients to build the $3\\times 4$ homography matrix $H$ we were looking for\n$$ \\begin{equation} H=K\\cdot E = \\begin{bmatrix} h_{11} \u0026amp; h_{12} \u0026amp; h_{13} \u0026amp; h_{14}\\\\ h_{21} \u0026amp; h_{22} \u0026amp; h_{23} \u0026amp; h_{24}\\\\ h_{31} \u0026amp; h_{32} \u0026amp; h_{33} \u0026amp; h_{34}\\\\ \\end{bmatrix} \\end{equation} $$\nwhich will allow us to project points from the 3D world into a 2D image\n$$ \\begin{equation} \\vec{P_H}=H\\cdot\\vec{p_H} \\end{equation} $$\nor if we expand it\n$$ \\begin{equation} s\\cdot \\begin{bmatrix} X \\ Y \\ 1 \\ \\end{bmatrix}=\\begin{bmatrix} f_x \u0026amp; 0 \u0026amp; \\frac{W}{2}\\\\ 0 \u0026amp; f_y \u0026amp; \\frac{H}{2}\\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} r_{11} \u0026amp; r_{12} \u0026amp; r_{13} \u0026amp; T_x\\\\ r_{21} \u0026amp; r_{22} \u0026amp; r_{23} \u0026amp; T_y\\\\ r_{31} \u0026amp; r_{32} \u0026amp; r_{33} \u0026amp; T_z\\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\ \\end{bmatrix} \\end{equation} $$\nFrom this definition, we can notice that the process is irreversible, since we can not invert a $3\\times 4$ matrix. However, the homography transform can also be used to characterize the mapping between two different 2D planes. This arises when we take two photographs of the same scene from different camera angles for instance, as illustrated below:\nExample of two different 2D views of the same 3D world object captured by two different cameras. So what can we do in this scenario? Remember we have chosen arbitrarily how to define our coordinate systems. Thus, we can rearrange the 3D world coordinate system to conveniently align with one of the images, such that the z-axis is orthogonal to it.\nWe can choose arbitrary the world coordinate system to match the camera coordinate system of one of them. This way, all the pixels from its captured 2D image correspond to 3D locations with $z=0$ in its camera coordinate system. As a result, all points in the image will have $z=0$,\n$$ \\begin{equation} s\\cdot \\begin{bmatrix} X \\\\ Y \\\\ 1 \\\\ \\end{bmatrix}= \\begin{bmatrix} h_{11} \u0026amp; h_{12} \u0026amp; h_{13} \u0026amp; h_{14}\\\\ h_{21} \u0026amp; h_{22} \u0026amp; h_{23} \u0026amp; h_{24}\\\\ h_{31} \u0026amp; h_{32} \u0026amp; h_{33} \u0026amp; h_{34}\\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\\\ 0 \\\\ 1 \\end{bmatrix} \\end{equation} $$\nwhich allows us to drop the third column in the homography matrix:\n$$ \\begin{equation} s\\cdot \\begin{bmatrix} X \\\\ Y \\\\ 1 \\\\ \\end{bmatrix}= \\begin{bmatrix} h_{11} \u0026amp; h_{12} \u0026amp; h_{14}\\\\ h_{21} \u0026amp; h_{22} \u0026amp; h_{24}\\\\ h_{31} \u0026amp; h_{32} \u0026amp; h_{34}\\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} x \\\\ y \\\\ 1 \\\\ \\end{bmatrix} \\end{equation} $$\nleading to a $3\\times 3$ homography matrix that. Notably, there is no longer a compression since it is a 2D‚Üí2D mapping. Consequently, under certain circumstances (mathematically, this implies $H$will be invertible), this transform can be reversible, allowing to convert back and forth between both images.\n5. References Richard Hartley and Andrew Zisserman. (2000), Page 143,¬†Multiple View Geometry in Computer Vision, Cambridge University Press. Wikipedia.¬†Pinhole camera model Dissecting the Camera Matrix: Parts I, II, III, by Kyle Simek. Camera intrinsics: Axis skew, b.y Ashima Athri. Concepts, Properties, and Usage of Homogeneous Coordinates, by Youngshon Zhao What are Intrinsic and Extrinsic Camera Parameters in Computer Vision? By Aqeel Anwar Camera Intrinsic Matrix with Example in Python, by Neeraj Krishna ","permalink":"http://localhost:1313/posts/projective_geometry/buiding_homograpahy_matrix/","summary":"Table of Contents 1. Pinhole camera model 2. Intrinsic matrix 2.1. Setup 2.2. Homogeneous coordinates 2.3. Accounting for distortions 2.3.1. Digital images 2.3.2. Rephotographing Images 3. Extrinsic matrix 4. Homography matrix 5. References 1. Pinhole camera model When we capture something on camera, there is an interesting phenomenon going on: compression. We are taking a photograph of a 3D world, and capturing it in a 2D image. This 3D‚Üí2D space mapping inevitably leads to information loss.","title":"üìΩÔ∏è Projective Geometry: Building the Homography Matrix from scratch"},{"content":" Hi! I'm I√±aki Rabanillo. I'm a Machine learning engineer/researcher with a PhD in Image Processing, currently working in the fields I love the most: sports. ","permalink":"http://localhost:1313/about/","summary":" Hi! I'm I√±aki Rabanillo. I'm a Machine learning engineer/researcher with a PhD in Image Processing, currently working in the fields I love the most: sports. ","title":"About Me"}]